{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Ryan_Mecking_LS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsmecking/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module1-rnn-and-lstm/Ryan_Mecking_LS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekXMfym-VFv3",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbI8ZFy-TKJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_fwf('https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBHwiEgcbnq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6ad3cd1f-cb11-4a74-af71-2cc9525fd9fb"
      },
      "source": [
        "data"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>First Citizen:</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Before we proceed any further, hear me speak.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>All:</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Speak, speak.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>First Citizen:</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You are all resolved rather to die than to fam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32771</th>\n",
              "      <td>And yet so fast asleep.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32772</th>\n",
              "      <td>ANTONIO:</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32773</th>\n",
              "      <td>Noble Sebastian,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32774</th>\n",
              "      <td>Thou let'st thy fortune sleep--die, rather; wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32775</th>\n",
              "      <td>Whiles thou art waking.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32776 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          First Citizen:\n",
              "0          Before we proceed any further, hear me speak.\n",
              "1                                                   All:\n",
              "2                                          Speak, speak.\n",
              "3                                         First Citizen:\n",
              "4      You are all resolved rather to die than to fam...\n",
              "...                                                  ...\n",
              "32771                            And yet so fast asleep.\n",
              "32772                                           ANTONIO:\n",
              "32773                                   Noble Sebastian,\n",
              "32774  Thou let'st thy fortune sleep--die, rather; wi...\n",
              "32775                            Whiles thou art waking.\n",
              "\n",
              "[32776 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJjEdenVVKky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data['First Citizen:'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61_0f1WlbJxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d0ae7bc-c7fe-4cf5-ba69-0b6eb9827bd0"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBzvWKUybJvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Gather all text \n",
        "# Why? 1. See all possible characters 2. For training / splitting later\n",
        "text = \" \".join(data)\n",
        "\n",
        "# Unique Characters\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4zrfD6tbJsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a50d59e-d103-43c7-e034-bcf9c94c38d5"
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM-uVIpkc9oY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84c7751e-b8b0-4e17-c3b4-2b6d9e540103"
      },
      "source": [
        "# Create the sequence data\n",
        "\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences:  221611\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqOz1kXJc9lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create x & y\n",
        "\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "        \n",
        "    y[i, next_char[i]] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDuGLa5ueMXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='nadam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MHAFbRCeMUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3rbuWm2eMSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    print()\n",
        "\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbR3xy50eX-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b1ac120-475b-4ea5-8cd0-c3490a49ac37"
      },
      "source": [
        "# fit the model\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=256,\n",
        "          epochs=10,\n",
        "          validation_split=.2,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "689/693 [============================>.] - ETA: 0s - loss: 1.4718\n",
            "----- Generating text after Epoch: 0\n",
            "----- Generating with seed: \"my faith, but the gleek; I will give you\"\n",
            "my faith, but the gleek; I will give you! But to let blood which thou, this is yours, rist. CORIOLANUS: The nights, by and all hame and doul!-- YORK: And thou are him God barfle. Chencether on the truitou and repors offer hald; I call not, whe had stents are not and Henry quich on your overue from as that, and that hampaser have, I'll in thke hast so cloth: but'st our this sibst, But tootstatch like of your bleny. HOMUMENER: LADy ; wars\n",
            "693/693 [==============================] - 14s 20ms/step - loss: 1.4716 - val_loss: 1.7914\n",
            "Epoch 2/10\n",
            "691/693 [============================>.] - ETA: 0s - loss: 1.4491\n",
            "----- Generating text after Epoch: 1\n",
            "----- Generating with seed: \"ld put breath into his work, would begui\"\n",
            "ld put breath into his work, would beguins with stome: Your braves but We may from these? My father have so and upplend on seezent. CORIOLANUS: Glord, think, riy master four pease of hear! Hon is off drie, good say, O, my life, my queer somesuliness to 'trands to drieves. RETERE O: The dosin revecter ip trelent do rupe: Durgles thou doth not scile and mother' threrexp up hat's Prince in York, hing lass Till me do printle in hele bight t\n",
            "693/693 [==============================] - 14s 21ms/step - loss: 1.4486 - val_loss: 1.7885\n",
            "Epoch 3/10\n",
            "692/693 [============================>.] - ETA: 0s - loss: 1.4378\n",
            "----- Generating text after Epoch: 2\n",
            "----- Generating with seed: \"name myself. AUFIDIUS: What is thy name?\"\n",
            "name myself. AUFIDIUS: What is thy name? KING EDWARD IV: Madiner: Now, too, eyes, complase, where she Voliness amon these mine of up is bards, Where yeullh. MARCIUS: Nay, noy with that, well; This east oldiunt so them graces your busifules. MENENIUS: Sir, juke it, you thrusk all you had fourth of look'd than foulds, thought as mudid Woolingaret? Markanus, voocioun on Shall gives of the worth's sesw. FRIAR LAURENCE: Why have cansong a se\n",
            "693/693 [==============================] - 14s 20ms/step - loss: 1.4377 - val_loss: 1.7904\n",
            "Epoch 4/10\n",
            "687/693 [============================>.] - ETA: 0s - loss: 1.4288\n",
            "----- Generating text after Epoch: 3\n",
            "----- Generating with seed: \" less. Nurse: No less! nay, bigger; wome\"\n",
            " less. Nurse: No less! nay, bigger; wome at the wret he's conjustiens in the lords, So shall him sir, I know thy compfet'st great Whith the brows me with our talour foot and beacious That love we faid? ROMEO: I house behought many mean? KING RICHARD III: I have wasterly Roman, Copeous and should Leves you again; tabe? Amfore Norfulis, so dray slowing; I have brese meechscies a beithion. EXENIUS: The wecting try than serviding were withi\n",
            "693/693 [==============================] - 14s 20ms/step - loss: 1.4291 - val_loss: 1.7935\n",
            "Epoch 5/10\n",
            "693/693 [==============================] - ETA: 0s - loss: 1.4200\n",
            "----- Generating text after Epoch: 4\n",
            "----- Generating with seed: \"rested and carried to prison was worth f\"\n",
            "rested and carried to prison was worth from Any have murder! If compal smacking sarring ouch ladies; Which well in this. ROMEO: Stanke boon peaces to sevfll it them, O' right? Edward will I ne't here is to her. Even so Chrigh you paty And lost, if is the king: I brantth, the veiding of her of good of wither, Buris no? Which we come word,' edimed, be there's spoked--Her deep it, and leak'd Recons friends Rife beghoy, all that rutise? WAR\n",
            "693/693 [==============================] - 14s 21ms/step - loss: 1.4200 - val_loss: 1.7909\n",
            "Epoch 6/10\n",
            "687/693 [============================>.] - ETA: 0s - loss: 1.4108\n",
            "----- Generating text after Epoch: 5\n",
            "----- Generating with seed: \"entence of your moved prince. Three civi\"\n",
            "entence of your moved prince. Three civil one omet-solrow, We shall side love begin, his coust There reation, I pareful fuily nothand that Hast bear thee out breath-somet-broft? The still and ever Romeo of tongue of his grieve. AUFOLIUS: To Roubloonas, there's depart mancen selves lead; That afcray him anoly upon the dother bettred. FLORIZEL: I will a breath of York is to time need. LEONTES: What, latiel trople. TORD: No brave! MERCUTIO\n",
            "693/693 [==============================] - 14s 21ms/step - loss: 1.4106 - val_loss: 1.7967\n",
            "Epoch 7/10\n",
            "693/693 [==============================] - ETA: 0s - loss: 1.3998\n",
            "----- Generating text after Epoch: 6\n",
            "----- Generating with seed: \"And then, as we have ta'en the sacrament\"\n",
            "And then, as we have ta'en the sacrament. BENVOLIO: My Lord as I have long more braother laps A closs dark framest will get their optncoldatter. Call I mutinament, how. CORIOLANUS: I'll that crsuch vieta it, and you welcome dound. MENENIUS: I have prove, eyeh thee, liks, I stalk'st newsitne: I'll let natering brots, when I hat too my cansems, writ the worst's heart. Clight o' the cart their menty and think bay by shall great And st our \n",
            "693/693 [==============================] - 14s 20ms/step - loss: 1.3998 - val_loss: 1.7978\n",
            "Epoch 8/10\n",
            "690/693 [============================>.] - ETA: 0s - loss: 1.3884\n",
            "----- Generating text after Epoch: 7\n",
            "----- Generating with seed: \"mfort; I cry bail. Here's a gentleman an\"\n",
            "mfort; I cry bail. Here's a gentleman and acselve my breagh. Parns, so I done enought If our counce, hage you, the false Here dony. But who helm-prove you was twond issape it is greas; They are father In art from sinces the lady; is it good dear: Let you is torded with heary. 'dive me by the mann'd comes or peetling; Exeorthing be his frilf, peace, for my good me less us, being not; revence, leadione. I hees'd queen be fellower stard In\n",
            "693/693 [==============================] - 14s 21ms/step - loss: 1.3889 - val_loss: 1.8007\n",
            "Epoch 9/10\n",
            "693/693 [==============================] - ETA: 0s - loss: 1.3767\n",
            "----- Generating text after Epoch: 8\n",
            "----- Generating with seed: \" shall answer it. BUCKINGHAM: What says \"\n",
            " shall answer it. BUCKINGHAM: What says your enewilign, you both will it with he me: Made worms well you sho lives light I should by liverass wordh. DUCHESS: But are yoy mersedverect is done to the years O; 'Tis vooke him to a hand by some with good Froke horbrough and greath. LIVIOLIN: Ay, soud to mine o' forthing I hmade do light. QUEEN ELIZABETH: All: then I am set the viceSations be not but see: We have great diemot by too murden my\n",
            "693/693 [==============================] - 14s 20ms/step - loss: 1.3767 - val_loss: 1.8039\n",
            "Epoch 10/10\n",
            "685/693 [============================>.] - ETA: 0s - loss: 1.3644\n",
            "----- Generating text after Epoch: 9\n",
            "----- Generating with seed: \"night immediately. Spread thy close curt\"\n",
            "night immediately. Spread thy close curtal dead, and blood Lest Upond thy gillan'd stainsble the child is their sugnest. The full to let up nor of our consilang Our gare teed of their thought than sware; SOMUSSS: Hookens, I speet not they; Engling stay is come-- FRIAR LAUREN E: Well, but you so hem finst now for with, Upunat, with his peaces what to pied from Thy figh the lay'd-- SAMESTE: But I hopry cleckery as thou shall perny, so dis\n",
            "693/693 [==============================] - 14s 20ms/step - loss: 1.3650 - val_loss: 1.8140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc3684af7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwyunIHSeX8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFNJSmlMH_9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUjalfxiH_3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYVNST1GH_vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}